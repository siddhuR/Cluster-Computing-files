Spark is also known as Lightning-fast unified analytics engine for big-data and machine learning.

Apache Hadoop is a batch processing framework. it keeps everything in secondary storage HDFS.
For every iteration, the data will get fetched from disk. its costly and time consuming.

spark: 
In-memory computation,
Ease of use
Spark is batch processing, graph processing, stream processing and iterative processing.

Ecosystem of spark:
Kafka, MySQL, PostgreesSQL, mongoDB, HBASE, HDFS, redis, elastic etc..

Components and API stack of Spark:
1. SparkSQL and DF + Datasets (Structured data)
2. Spark Streaming (Structured Streaming)(for real time)
3. Machine Learning MLlib
4. Graph Processing Graph X (Dealing with network data)

Spark Core and Spark SQL Engine
Scala, SQL, Python, Java, R

We can use spark on YARN, MESOS
===========

Hadoop frame work is composed of
 - Hadoop Common
 - Hadoop Distributed File System
 - Hadoop YARN
 - Hadoop MapReduce
Non core components Like
 - Hive
 - Flume
 - YARN
 - HBase
 - pig etc

===========

Spark code:

import.org.apache.spark.sql.SparkSession

object obj1 {
	def main(args: Array[string])
	{
		val spark = SparkSession.builder().master("local[1]").appName("abc").getOrCreate();

println("Spark Version : " + spark.version)

}























