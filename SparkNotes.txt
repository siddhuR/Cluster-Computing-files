total 4 context 
		1)Spark Context
		2)SQL ""
		3)Steaming
		4)Hive
all together called spark session

Scala have 3 prefixes-
		1. Var (mutable)  ,var a=5
		2.Val	(immutable) ,val b="hello"
		3.Lazy  (lazy evaluation if we want anything to delay) ,lazy val c=20  
									it is not assigned yet when we write c and hit enter then it will asign

==========================================

=> SCALA // basic commands

>> val a = 5
a: Int = 5

>> a = 7	// gives error
<console>:23: error: reassignment to val
       a = 7
         ^

>> val b = "HELLO"
b: String = hello

>> lazy val c = 7
c: Int = <lazy>

>> c
res0: Int = 7

==========================================

# OPERATORS IN SCALA

ARITHMETIC OPERATORS:

>> val operand1 = 10
>> val operand2 = 7

>> println(operand1 + operand2)
>> println(operand1 - operand2)
>> println(operand1 * operand2)
>> println(operand1 / operand2)
>> println(operand1 % operand2)

RELATIONAL OPERATORS:

>> val operand1 = 10
>> val operand2 = 7

>> println(operand1 > operand2)
>> println(operand1 < operand2)
>> println(operand1 >= operand2)
>> println(operand1 <= operand2)
>> println(operand1 != operand2)

LOGICAL OPERATORS:

>> val A = true
>> val B = false
>> val exp = A && B

BITWISE OPERATOR:

>> val A = 12
>> val B = 5

>> println(~A)
>> println(~B)
>> println(A & B)
>> println(A | B)
>> println(A ^ B)

ASSIGNMENT OPERATORS:

>> vae A = 10
>> var B = 7

>> print("Before using an assignment operator: ")
>> println(A)

>> A += B

>> print("After using an assignment operator: ")
>> println(A)

===============================================

>> stringVar = "what is the length of this string?"
>> println(stringVar.length())

======

>> println("Hello" + "World")

>> val string1 = "Hello"
>> val string2 = "World"
>> print(string1 + string2)

===============================================


RDD (RESILIENT DISTRIBUTED DATASET)


	3 ways to create RDD:
>> PARALLELIZE METHOD

sc.parallelize(scala_collection)

>> TEXTFILE METHOD:

sc.textFile("")

>> USING EXISTING RDD



==============================================

Collection library:

val seqCollection = Seq(2,4,6,8)
val result = seqCollection.apply(1)
print(result)	// ans : // 4

val setCollection = Set("Apple", "Orange", "Banana", "Grape")
val result1 = setCollection.apply("Orange")
println(result1)	// ans : // true

val mapCollection = Map(("a",25),("b",50),("c",75))
val result2 = mapCollection.apply("c")
println(result2)	// ans : // 75


==============================================

# Creation of data frame from existing data

val g = sc.parallelize(Array(("a",1),("a",2))).toDF()	// to covert rdd to dataframe

val df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/home/siddhu/workspace/Input_Files/dataframe.csv")	//to convert different type of data into data frame like json,csv etc.

df.show()	// to print the data present in the csv file





















